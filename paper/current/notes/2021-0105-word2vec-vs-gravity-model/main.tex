\documentclass[12pt]{article} %{{{

% Figures
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\def\figdir{figs}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\def\given{\mid}

% abbreviations
\def\etal{\emph{et~al}.\ }
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{newtxmath}
\DeclareMathAlphabet{\mathpzc}{T1}{pzc}{m}{it}
\usepackage{bm}
\def\tnull{{\text{null}}}
\def\vec#1{{\bm #1}}
\def\mat#1{\mathbf{#1}}

% Refs
\usepackage{biblatex}
\addbibresource{main.bib}

\usepackage{url}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
%\newcommand{\eqnref}[1]{\eqref{eq:#1}}
%\newcommand{\thmref}[1]{Theorem~\ref{#1}}
%\newcommand{\prgref}[1]{Program~\ref{#1}}
%\newcommand{\algref}[1]{Algorithm~\ref{#1}}
%\newcommand{\clmref}[1]{Claim~\ref{#1}}
%\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\ptyref}[1]{Property~\ref{#1}}

% for quick author comments 
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{light-gray}{gray}{0.8}
\def\del#1{ {\color{light-gray}{#1}} }
\def\yy#1{ {\color{red}\textbf{yy: #1}} }

%}}}

\begin{document} %{{{

\title{Gravity model and \textit{\textit{word2vec}}} %{{{
\date{\today}
\maketitle %}}}

\section{From \textit{word2vec} to gravity model}

The \textit{word2vec} fits well to a wide range of mobilities, which prompts us to ask: why does the \textit{word2vec} fit well to the mobility data?
It turns out that the \textit{word2vec} is tied to a gravity model, a widespread and long-standing model that well explains human mobility.
While the gravity model predicts a flow from given locations and populations, the \textit{word2vec} is a reverse engineering model that estimates the location and population from flows.

Let us approach the connection by initially focusing on the gravity model.
Suppose that flow of people between $N$ locations. The gravity model models the flow by
\begin{align}
    \hat{T_{ij}} = C m_i m_j f(r_{ij}), \label{eq:gravity_model}
\end{align}
where $\hat{T_{ij}}$ is the expected flow from location $i$ to $j$, $m_i$ is the population of locations of $i$, $r_{ij}$ is a distance between locations $i$ and $j$, $f$ is a decreasing function with respect to $r_{ij}$, and $C$ is a constant.
That the gravity model implicitly assumes that the flow is symmetric, \ie $\hat{T_{ij}}=\hat{T_{ji}}$, which ensures the stationarity in population $m_{i}$.
The choice of distance measure $d_{ij}$ is entirely unconstrained; it can be the euclidean distance, or any kind of general distance measure.
In the following, we refer to a measure of dissimilarity as distance.

Now, let us move our focus to the \textit{\textit{word2vec}}.
In out setting, the model takes a sequence of locations and models the co-occurrence of center location $i$ and context location $j$ by
\begin{equation}
p(j \given i) = \frac{\exp(\bm{u}_j \cdot \bm{v}_{i})}{Z_i},
\end{equation}
where $\mathcal{A}$ is the entire set of unique locations represented in the data, and  $\bm{v}$ and $\bm{u}$ are the ``in-vector" and ``out-vector" respectively.  $Z_i=\sum_{x \in \mathcal{A}} \exp(\bm{u}_{x} \cdot \bm{v}_{i})$ is a normalization constant. Under the stationary condition and restricting window size to be one , the expected flow between center location $a_i$ and context location $a_j$ is given by
\begin{align}
	  \label{eq:flow_w2v}
        \hat{T_{ij}}=\pi_i P(j\given i) = \frac{\pi_i\exp(\bm{u}_j \cdot \bm{v}_{i})}{Z_i},
\end{align}
where $\pi_i$ is the frequency of location $i$ over the dataset

Interestingly, Eq.~\ref{eq:flow_w2v} has a similar functional form with the gravity model, \ie $d_{ij} = - \bm{u}_{j} \cdot \bm{v}_{i}$ and $f(x) = \exp(-x)$.
Yet, the Eq.~\ref{eq:flow_w2v}  does not have a population term $m_i$, and the distance can be asymmetric,$d_{ij}=- \bm{u}_{j} \cdot \bm{v}_{i} \neq - \bm{u}_{i} \cdot \bm{v}_{j}=d_{ji}$.
In practice, however, this gap is filled due to the effect of optimization algorithm---\emph{the negative sampling}.

The negative sampling is an optimization method widely used for training the \textit{word2vec}, which operates as follows.
For each center--context word pair $(i,j)$, one generates $k$ noise center--context word pairs, denoted by $(i, \ell)$, where context word $\ell$ for the noise pair is sampled from a noise distribution, denoted by $p_{0}(\ell)$.
Given a center-context word pair, $(i,j)$, as an input, one predicts whether it comes from the original data or noise distribution based on
a similarity between two words, \eg dot similarity in case of the \textit{word2vec}.

The negative sampling is a biased estimator that optimizes a slightly different objective function~\autocite{Gutmann2010}, not directly fit the \textit{word2vec} with the the synthetic classification task.
As a consequence, the model optimized following conditional probability,
\begin{align}
    P\left(j \given i \right):= \frac{p_0(j)\exp(\bm{u}_j \cdot \bm{v}_{i})}{Z'_i}, \label{eq:prob_w2v_ng0}
\end{align}
where $Z'_i=\sum_{x \in \mathcal{A}} p_0(x) \exp(\bm{u}_{x} \cdot \bm{v}_{i})$ is a new biased normalization constant.

The \textit{word2vec} uses a noise distribution given by $p_{0}(j) = \pi^{\gamma} _j / \sum_{\ell'} \pi^{\gamma}_{\ell'}$, where $\gamma$ is a free parameter typically set to $3/4$ or $1$. Notably, $\gamma=1$ is a special choice that ensures that the in-vector and out-vector are the same, \ie $u_i = v_i$ \autocite{levy2014neural}. Setting $\gamma = 1$ and substituting $p_{0}(j) = \pi^{\gamma} _j / \sum_{\ell'} \pi^{\gamma}_{\ell'}$ into Eq~\eqref{eq:flow_w2v_ng0} yields

\begin{align}
    P\left(j \given i \right):= \frac{\pi^{\gamma} _j  \exp(\bm{u}_j \cdot \bm{v}_{i})}{ \sum_{\ell'} \pi^{\gamma}_{\ell'} Z'_i} =  \frac{\pi _j  \exp(\bm{v}_j \cdot \bm{v}_{i})}{ \sum_{\ell'} \pi_{\ell'} Z'_i},
\end{align}

\begin{align}
    \hat{T_{ij}}=\pi_i P(j\given i) =  \frac{\pi_i \pi _j  \exp(\bm{v}_j \cdot \bm{v}_{i})}{ \sum_{\ell'} \pi_{\ell'} Z'_i}. \label{eq:flow_w2v_ng0}
\end{align}

Also, deriving form $\hat{T_{ij}}=\hat{T_{ji}}$ yields
\begin{align}
    \frac{\pi_i \pi _j  \exp(\bm{v}_j \cdot \bm{v}_{i})}{ \sum_{\ell'} \pi_{\ell'} Z'_i} =  \frac{\pi_j \pi _i  \exp(\bm{v}_i \cdot \bm{v}_{j})}{ \sum_{\ell'} \pi_{\ell'} Z'_j},
\end{align}
\begin{align}
    Z'_j = Z'_i. \label{eq:euqal_rel}
\end{align}

Combining Eq. \eqref{eq:flow_w2v_ng0} and Eq. \eqref{eq:euqal_rel} yields
\begin{align}
\hat{T_{ij}}=\pi_i P(j\given i) =  C \pi_i \pi _j  \exp(\bm{v}_j \cdot \bm{v}_{i}),
\end{align}
where $C$ is a positive constant.
Eq.~\eqref{eq:flow_w2v_ng0} reveals the connection between the \textit{word2vec} and the gravity model Eq.~\eqref{eq:gravity_model} with $d_{ij} = - \bm{u}_{j} \cdot \bm{v}_{i}$ and $f(x) = \exp(-x)$.
For example, the \textit{word2vec} is a special case of the gravity model, in which the mass of location $i$ is its frequency in the trajectory data, and the distance between locations is measured by their dot similarities.

\section{From gravity model to \textit{word2vec}}
In gravity model,  the probability that a person moves from location i to j is given by
\begin{align}
P\left(j \given i \right) = T_{ij} / m_i = C m_j f(d_{ij}). \label{eq:gravity_prob}
\end{align}

Because, the sum of conditional probability should be one,
\begin{align}
 C \sum_{k} m_k f(d_{ik}) = 1, \label{eq:gravity_constant}
\end{align}
which means $\sum_{j} m_j f(d_{ij})$ is constant. Substituting Eq.~\eqref{eq:gravity_prob} into \eqref{eq:gravity_constant} yields
\begin{align}
P\left(j \given i \right) = \frac{m_i f(d_{ij})}{\sum_{k} m_j f(d_{ik})}. \label{eq:gravity_prob},
\end{align}
which is similar functional form of  Eq.~\eqref{eq:prob_w2v_ng0}.






%\section{Optimization based on the multidimensional scaling}
%
%The euclidean \textit{word2vec} takes flow as input and outputs the location and population at each location.
%This algorithm is reminiscent of the multidimensional scaling (MDS), which maps entities onto a Euclidean space by preserving their dissimilarity as distance as much as possible.
%Specifically, given a dissimilarity matrix $D=(D_{ij})$, MDS aims to map entities such that $D_{ij} \approx ||u_i - u_j||$ for all $i$ and $j$.
%This connection allows us to optimize the euclidean \textit{word2vec} using the MDS algorithm, as we will see in the following.
%
%For a moment, let us consider an ideal situation in which the given data is generated from Eq~\eqref{eq:flow_w2v_ng} and how we can fit the model.
%Denoted by $T_{ij} ^\text{data}$ as the actual flow, and $T^{\text{model}}_ij$ by the flow modeled by the \textit{word2vec}.
%When embedding dimension $K$ is small, the \textit{word2vec} has a limited expression power and thus the modeled flow can be considerably different to the actual flow.
%As we increase $K$, however, the model gains more expression power and eventually achieves an exact fit, $T_{ij} ^\text{data} = T_{ij} ^\text{model}$, just like fitting a high-order polynomial function (\eg quadratic function) to two data points.
%In this case, the distance is given by solving Eq.~\eqref{eq:flow_w2v_ng}, or
%\begin{align}
%    ||u_i - u_j|| = - \log \frac{T^\text{data}_{ij}}{\pi_i \pi_j} + \log c.
%\end{align}
%This problem is equivalent to the MDS problem, taking $R=(R_ij)$ with entries $R_{ij}= - \log \frac{T_{ij}}{\pi_i \pi_j} + \log c$ as the dissimilarity matrix.
%It should be noted that MDS problem does not have a unique solution.
%
%In practice, we may want a compact embedding space, $K\ll N$.
%Furthermore, given data may not be generated from the same model.
%Yet, solving the MDS problem would provide a good approximate for embedding.
%
%An implementation issue is that the dissimilarity $R_{ij}$ can be infinity because flow $T_{ij}$ can be zero, making the MDS problem invalid.
%To circumvent this problem, we add a small synthetic flow $\tilde \epsilon_{ij}$ to the actual flow given by $\epsilon_{ij} = \alpha \pi_i \pi_j$, where $\alpha = 1e-3$.

\printbibliography{}

\end{document} %}}}
