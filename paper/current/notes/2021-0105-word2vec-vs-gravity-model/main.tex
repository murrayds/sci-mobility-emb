\documentclass[12pt]{article} %{{{

% Figures
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\def\figdir{figs}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\def\given{\mid}

% abbreviations
\def\etal{\emph{et~al}.\ }
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }

% Refs
\usepackage{biblatex}
\addbibresource{main.bib}

\usepackage{url}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
%\newcommand{\eqnref}[1]{\eqref{eq:#1}}
%\newcommand{\thmref}[1]{Theorem~\ref{#1}}
%\newcommand{\prgref}[1]{Program~\ref{#1}}
%\newcommand{\algref}[1]{Algorithm~\ref{#1}}
%\newcommand{\clmref}[1]{Claim~\ref{#1}}
%\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\ptyref}[1]{Property~\ref{#1}}

% for quick author comments 
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{light-gray}{gray}{0.8}
\def\del#1{ {\color{light-gray}{#1}} }
\def\yy#1{ {\color{red}\textbf{yy: #1}} }

%}}}

\begin{document} %{{{

\title{Gravity model and word2vec} %{{{
\date{\today}
\maketitle %}}}

\section{Introduction}\label{sec:introduction}

The word2vec fits well to a wide range of mobilities, which prompts us to ask: why does the word2vec fit well to the mobility data?
It turns out that the word2vec is tied to a gravity model, a widespread and long-standing model that well explains human mobility.
While the gravity model predicts a flow from given locations and populations, the word2vec is a reverse engineering model that estimates the location and population from flows.

Let us approach the connection by initially focusing on the gravity model.
Suppose that flow of people between $N$ locations. The gravity model models the flow by
\begin{align}
    T_{ij} = G M_i M_j f(d_{ij}), \label{eq:gravity_model}
\end{align}
where $T_{ij}$ is the flow from location $i$ to $j$, $M_i$ is a mass term corresponding to population of $i$, $d_{ij}$ is a distance between locations $i$ and $j$,
$f$ is a decreasing function with respect to $d_{ij}$, and $G$ is a constant.
Note that the gravity model implicitly assumes that the flow is symmetric, \ie $T_{ij}=T_{ji}$, which ensures the stationarity in population $M_{i}$.

Now, let us move our focus to the word2vec.
The word2vec takes a sequence of locations (\ie trajectory) and models the co-occurrence of center word $i$ and context word $j$ by
\begin{align}
    P(j\given i) = \frac{\exp(u_i ^\top v_j)}{Z_i},
\end{align}
where $u_i$ and $v_{j}$ are embedding vectors for node $i$, which are referred to as in-vector and out-vector, respectively, and $Z_i$ is a normalization constant.
The co-occurrence \emph{frequency} of center and context word is given by
\begin{align}
    \pi_i P(j\given i) = \frac{\pi_i\exp(u_i ^\top v_j)}{Z_i},
\end{align}
where $\pi_i$ is the frequency of word $i$ in the sentence. When restricting window size to be one, the co-occurrence frequency corresponds to the flow, \ie
\begin{align}
    T_{ij} = \frac{\pi_i\exp(u_i ^\top v_j)}{Z_i}. \label{eq:flow_w2v}
\end{align}
Additionally, because the window covers the preceding and succeeding words from a center word, the co-occurrence is symmetric, $T_{ij} = T_{ji}$, which echoes the gravity model.
Exploiting the symmetry in flows (\ie $T_{ij} = T_{ji}$), we have
\begin{align}
    \frac{\pi_i}{Z_i}  = \frac{\pi_j}{Z_j}
    \Rightarrow Z_i   = \pi_i / c \label{eq:Z}
\end{align}
where $c$ is a positive constant. Substituting \eqref{eq:Z} into \eqref{eq:flow_w2v}, we have
\begin{align}
    T_{ij} = c \exp(u_i ^\top v_j). \label{eq:flow_w2v2}
\end{align}
Both word2vec and the gravity model assume that the flow is determined by the locations.
Yet, they have some notable differences.
For example, the gravity model measures the distance by the euclidean distance while the word2vec measures by a dot similarity.
Additionally, the word2vec does not have a mass term for locations.

%The word2vecs flow between locations based on a dot similarity while the gravity model does baesd on eucliden distance.
%Although the two models differ in many respects, 
The gap between two models can be filled by using euclidean distance for the word2vec, \ie
\begin{align}
    P(j\given i) = \frac{\exp(-||u_i - u_j||)}{Z_i}.
\end{align}
The flow is then given by
\begin{align}
    T_{ij} = c \exp(-||u_i - u_j||).
\end{align}
Now, this Euclidean word2vec has a functional form similar to the gravity model, \ie the flow is decreasing with respective to the euclidean distance.

Although the mass terms are still missing in the modified word2vec, they come into the model when using a commonly-used optimization algorithm---\emph{the negative sampling}.
The negative sampling is an optimization method widely used for training the word2vec.
An overlooked fact is that the negative sampling is a biased estimator that optimizes a slightly different objective function~\cite{Gutmann2010}.

The negative sampling finds the embedding vectors as follows.
For each center--context word pair $(i,j)$, one generates $k$ noise center--context word pairs, denoted by $(i, \ell)$, where context word $\ell$ for the noise pair is sampled from a noise distribution, denoted by $p_{0}(\ell)$.
Given a center-context word pair, $(i,j)$, as an input, one predicts whether it comes from the original data or noise distribution based on
a similarity between two words, \eg dot similarity in case of the word2vec.

Note that the negative sampling does not directly fit the word2vec to the given data but with the synthetic classification task.
As a consequence, it does not fit the original model but a different model given by \cite{Gutmann2010}
\begin{align}
    P\left(j \given i \right):= \frac{p_0(j)\exp(u^\top _i v_j)}{Z_i}.
\end{align}
Although the negative sampling optimizes a different model, it has computational advantages and works well in practice.

Let us return to the Euclidean word2vec and optimize it using the negative sampling.
The flow between $i$ and $j$ is given by
\begin{align}
    T_{ij}=\pi_i\frac{p_0(j)\exp(-||u_i - u_j||)}{Z_i}.
\end{align}
The original word2vec uses a noise distribution given by $p_{0}(\ell) = \pi^{\gamma} _\ell / \sum_{\ell'} \pi^{\gamma}_{\ell'}$, where $\gamma$ is a free parameter typically set to $3/4$ or $1$.
%We set $\gamma = 1$.
Setting $\gamma=1$, we have
\begin{align}
    T_{ij}=c\pi_i \pi_j\exp(-||u_i - u_j||). \label{eq:flow_w2v_ng}
\end{align}
which reveals the mass $M_i$ for the Euclidean word2vec model.
In fact, the mass term $M_i$ for location $i$ corresponds to its frequency in the trajectory in the Euclidean word2vec model.


\section{Optimization based on the multidimensional scaling}

The euclidean word2vec takes flow as input and outputs the location and population at each location.
This algorithm is reminiscent of the multidimensional scaling (MDS), which maps entities onto a Euclidean space by preserving their dissimilarity as distance as much as possible.
Specifically, given a dissimilarity matrix $D=(D_{ij})$, MDS aims to map entities such that $D_{ij} \approx ||u_i - u_j||$ for all $i$ and $j$.
This connection allows us to optimize the euclidean word2vec using the MDS algorithm, as we will see in the following.

For a moment, let us consider an ideal situation in which the given data is generated from Eq~\eqref{eq:flow_w2v_ng} and how we can fit the model.
Denoted by $T_{ij} ^\text{data}$ as the actual flow, and $T^{\text{model}}_ij$ by the flow modeled by the word2vec.
When embedding dimension $K$ is small, the word2vec has a limited expression power and thus the modeled flow can be considerably different to the actual flow.
As we increase $K$, however, the model gains more expression power and eventually achieves an exact fit, $T_{ij} ^\text{data} = T_{ij} ^\text{model}$, just like fitting a high-order polynomial function (\eg quadratic function) to two data points.
In this case, the distance is given by solving Eq.~\eqref{eq:flow_w2v_ng}, or
\begin{align}
    ||u_i - u_j|| = - \log \frac{T^\text{data}_{ij}}{\pi_i \pi_j} + \log c.
\end{align}
This problem is equivalent to the MDS problem, taking $R=(R_ij)$ with entries $R_{ij}= - \log \frac{T_{ij}}{\pi_i \pi_j} + \log c$ as the dissimilarity matrix.
It should be noted that MDS problem does not have a unique solution.

In practice, we may want a compact embedding space, $K\ll N$.
Furthermore, given data may not be generated from the same model.
Yet, solving the MDS problem would provide a good approximate for embedding.

An implementation issue is that the dissimilarity $R_{ij}$ can be infinity because flow $T_{ij}$ can be zero, making the MDS problem invalid.
To circumvent this problem, we add a small synthetic flow $\tilde \epsilon_{ij}$ to the actual flow given by $\epsilon_{ij} = \alpha \pi_i \pi_j$, where $\alpha = 1e-3$.

\printbibliography{}

\end{document} %}}}
