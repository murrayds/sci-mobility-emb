\documentclass[12pt,draft,a4paper]{article}
\usepackage{graphicx}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{pslatex}
\usepackage{color}
\usepackage{bm}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\definecolor{orange}{RGB}{230, 81, 0}
\definecolor{purple}{RGB}{170, 0, 255}
\definecolor{mygreen}{RGB}{76, 175, 80}
\usepackage{framed}
\newcommand{\vect}[1]{\boldsymbol{#1}}
\definecolor{shadecolor}{RGB}{210, 210, 210}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{newtxmath}
\DeclareMathAlphabet{\mathpzc}{T1}{pzc}{m}{it}
\def\given{\mid}
\usepackage{bm}
\def\tnull{{\text{null}}}
\def\vec#1{{\vect #1}}
\def\mat#1{\mathbf{#1}}

%
% Huxtable dependencies (see R package)
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{siunitx}
\usepackage{ulem}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{calc}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{wrapfig}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{romannum}

% Allow landscape rotation on table page
\usepackage{lscape}

% abbreviations
\def\etal{\emph{et~al}.\ }
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }



\usepackage{hyperref}

\usepackage[style=nature,
					backend=biber,
					sortcites=true,
					autocite=superscript
]{biblatex}

\AtEveryBibitem{%
 \clearfield{url}%
   \clearfield{month}%
 \clearfield{issn}%
 \clearfield{doi}%%
}


\addbibresource{rebuttal.bib}


%
% HEADER
%



\newcounter{comment}[subsection]


%
% COLORBOX
%
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\newtcolorbox[]{sectionbox}[1]{colback=white,
 				       												 colframe=black,
 				       												 colbacktitle=white!50!gray,
 				       												 coltitle=black,
 				       												 fonttitle=\bfseries,
 				       												 phantom=\refstepcounter{comment},
 				       												 title={Comment \arabic{section}.\arabic{comment}},
 				       												 list entry={Comment \arabic{section}.\arabic{comment}\quad}, breakable}


\DeclareUnicodeCharacter{FB01}{fi}
\usepackage{dirtytalk}

\def\figdir{../Figs}

%\usepackage[most]{tcolorbox}
\newcommand{\sada}[1]{{\leavevmode\color{orange}[SK: #1]}}
\newcommand{\sadaadded}[1]{{\leavevmode\color{orange}{#1}}}
\newcommand{\todo}[1]{{\leavevmode\color{orange}[TODO: #1]}}
\newcommand{\add}[1]{{\leavevmode\color{blue}#1}}
\newcommand{\del}[1]{{\leavevmode\color{red}\sout{#1}}}
\newcommand{\response}[1]{{\noindent \textbf{Response:} \\ \\ \noindent #1}}
%\newcommand{\rcomment}[1]{{\bigskip\begin{snugshade}\begin{quotation}\noindent #1 \end{quotation}\end{snugshade} }}

\newcommand{\rcomment}[1]{%
\vspace{10pt}
\begin{sectionbox}
s #1
\end{sectionbox}
}

\newcommand{\correction}[2]{{\begin{quotation}\noindent {#1}{\it #2} \end{quotation} }}
%\newcommand{\correction}[2]{{\bigskip\begin{quotation}\noindent {#1}{\it #2} \end{quotation} }}

\newcommand{\mycaption}[2]{%
  \caption[#1]{\textbf{#1} \small#2}%
}
% comment out when submitting
\definecolor{myblue}{RGB}{33,150,243}
\definecolor{mygreen}{RGB}{76, 175, 80}
\definecolor{purple}{RGB}{170, 0, 255}

\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
\Large  \textbf{\@title}
\end{flushleft}\egroup
}
\makeatother
\title{Re: Nature submission 2020-10-19515}
\date{}

\newgeometry{margin=1in, top=1cm}
\begin{document}
\maketitle

\bigskip
\thispagestyle{empty}
\vspace{-0.4cm}
\noindent Dear Dr. Sutherland and Referees,\\
\vspace{-0.25cm}

We thank all the referees for their thoughtful and helpful comments and critiques.
We acknowledge that our initial submission had a substantial gap regarding the choice of the \textit{word2vec} model, its relationship to the gravity model of mobility, and its evaluation.
While reflecting on these comments and our manuscript, we have made a theoretical breakthrough that addresses the fundamental issues, and dramatically expands the scope of our contribution;
namely, we have discovered \emph{a mathematical equivalence between the \textit{word2vec} (skip-gram with negative sampling) model and the gravity model.}

Furthermore, our subsequent experiments show that our approach's performance is robust, exceeding many other baselines, even direct optimization of the gravity model.
Therefore, we are confident that these changes have brought a qualitative leap in the significance of our findings.
Thus, we would like to request a reconsideration of our manuscript.
Again, we are incredibly appreciative of the referees' comments, which provided the insight and motivation needed to make these breakthroughs.

We have developed the mathematical and theoretical basis of our approach for embedding mobility, and made additional updates to our experiments and manuscript, addressing the three main issues raised by referees:
\vspace{-0.1cm}
\begin{enumerate}
	\itemsep0em
	\item The lack of theoretical justification and grounding for the application of \textit{word2vec}
	\item The lack of simpler baselines to understand the performance of \textit{word2vec}
	\item The lack of sufficient grounding in the literature of neural embeddings and mobility
\end{enumerate}
\vspace{-0.1cm}
We address the first point by presenting the mathematical equivalence between the gravity model of mobility and the skip-gram negative sampling \textit{word2vec} model.
Namely, we demonstrate that \textit{word2vec}'s objective function is equivalent to the gravity model due to the implicit bias of negative sampling,  and thus \textit{word2vec} learns gravity-like relationships between entities, whether it be language or locations.
We believe that this development constitutes a major breakthrough in our understanding of neural embeddings, and in addition to justifying their use in our study, also opens the door for further research applying them to the study of mobility.

For the second point, we have conducted experiments comparing the performance of neural embedding to (\romannum{1}) the singular value decomposition of the mobility network,  (\romannum{2}) a Laplacian eigenmap embedding  (\romannum{3}) a \textit{word2vec} model with matrix factorization,  and  (\romannum{4}) a direct optimization of the gravity model. We find that the embedding distance provided by \textit{word2vec} outperforms all baselines, including the direct optimization of gravity model with the multidimensional scaling, in explaining and predicting real-world mobility.

Finally, in order to address the third point, we have incorporated relevant literature to better ground our manuscript in the existing studies.

In the attached detailed response, we outline the connection between \textit{word2vec} and the gravity model (Section 1), and comparisons to baselines (Section 2). We then provide a point-by-point response to each of the reviewer's comments in Section 3.

We thank you for your time, attention, and insight, and look forward to hearing back from you!\\

Kind Regards,

Dakota, Jisung, Sadamori, Rodrigo, Woo-Sung, Staša, and YY

\pagenumbering{arabic}
\clearpage
\newgeometry{margin=1in}
\setcounter{page}{1}
\tableofcontents
\clearpage

\section{Theoretical justification of \textit{word2vec} and the Gravity Model}
\label{sec:theory}
\thispagestyle{empty}
Why does the \textit{word2vec} with skip-gram and negative sampling (SGNS) model work so well to model mobility?
Here, we demonstrate that SGNS {\it word2vec} and the gravity model of mobility are mathematically equivalent.
This connection provides strong theoretical justification to our choice of \textit{word2vec} to model mobility, and constitutes a major breakthrough of our understanding of neural embeddings.
In addition, this motivates the use of SGNS {\it word2vec} for the future study of human mobility.

\subsection{\textit{word2vec} algorithm}
Imagine a trajectory, denoted by ($a_{1}, a_{2}, \ldots, a_{T}$), where $a_{t}$ is the $t$\textsuperscript{th} location in the trajectory. A location, $a_{t}$, is considered to have context locations, $a_{t-w}, \ldots, a_{t-1}, a_{t+1},\ldots, a_{t+w}$, that appear in the window surrounding $a_t$ up to a time lag of $w$, where $w$ is the window size parameter truncated at $t - w \geq 0$ and $t + w \leq T$.
Then, the skip-gram \textit{word2vec} model learns probabilities $p(a_{t + \tau} \vert a_{t})$, where $-w\leq \tau\leq w$ and $\tau \neq 0$,  by maximizing its log likelihood given by
%
% MAIN W2V Equation
%
\begin{equation}
	{\cal J} = \frac{1}{T}\sum_{t = 1}^{T} \sum_{ \substack{-w \leq \tau \leq w, \\ 0 \leq t+\tau \leq T \\\tau \neq 0}} \log P(a_{t + \tau} \vert a_{t}), \label{eq:word2vec-obj}
\end{equation}
where

%
% CONDITIONAL PROBABILITY EQ
%
\begin{equation}
	P(j \given i) = \frac{\exp(\vect{u}_j \cdot \vect{v}_{i})}{Z_i}, \label{eq:cond_prob_w2v}
\end{equation}
where $\vect{v}$ and $\vect{u}$ are the ``in-vector'' and ``out-vector'', respectively,  $Z_i=\sum_{j' \in \mathcal{A}} \exp(\vect{u}_{j'} \cdot \vect{v}_{i})$ is a normalization constant, and $\mathcal{A}$ is the set of all locations. In general, calculating $Z_i$ is computationally expensive, so two kinds of approximations are commonly used:
hierarchical softmax \autocite{morin2005hierarchical} and negative sampling \autocite{mikolov2013word2vec}.
Due to its simplicity and performance, negative sampling is the most widely used strategy, which we also adopt in our study.


\subsection{An implicit bias in the negative sampling}

Although negative sampling is the most common approximation, it is a biased estimator~\autocite{Chia2010,Dyer2014} that does not yield the optimal embeddings in terms of the objective function, ${\cal J}$.
This bias in negative sampling turns out to be key to the connection between {\it word2vec} and the gravity model.

Negative sampling trains {\it word2vec} as follows.
For each target word $i$, we sample a context word $j$ from the given data and label it as positive, denoted by $Y_{j}=1$.
Then, we sample $k$ words $\ell$ from a noise distribution $p_0(\ell)$ and label them as negative, denoted by $Y_{\ell}=0$.
In \textit{word2vec}, the noise distribution is given by $p_0(\ell) \propto P^\gamma (\ell)$, where $P(j)$ is the fraction of $j$ in the data, and $\gamma$ is a hyper-parameter.
Then, for the sampled words, we fit a logistic regression model
\begin{align}
	\label{eq:logistic-regress}
	P^{\text{NS}}(Y_{j} = 1; \vect{v}_{i}, \vect{u}_j) = \frac{1}{1 + \exp(-\vect{u}_j \cdot \vect{v}_{i})},
\end{align}
by maximizing the log-likelihood:
\begin{align}
	\label{eq:log-likelihood-logistic-regress}
	{\cal J}^{\text{NS}} = \sum_{i \in {\cal A}} \sum_{j \in {\cal D}} \left[ Y_{j} \log P^{\text{NS}}(Y_{j} = 1; \vect{v}_{i}, \vect{u}_j) + (1-Y_{j}) \log P^{\text{NS}}(Y_{j} = 0; \vect{v}_{i}, \vect{u}_j)\right],
\end{align}
where ${\cal D}$ is the set of all sampled context words.

An underappreciated fact is that maximizing ${\cal J}^{\text{NS}}$ does not guarantee
to yield the optimum embedding in terms of the original skip-gram {\it word2vec} objective function ${\cal J}$ even if we increase the training samples and iterations~\autocite{Chia2010,Dyer2014}.
To make this bias explicit, let us consider the unbiased variant of negative sampling, \ie the noise contrastive estimation (NCE)~\autocite{Chia2010,Dyer2014}.
NCE is an unbiased estimator for a probability model $P_m$ of the form:
\begin{align}
	\label{eq:nce-model}
	P_m(x) = \frac{ f(x) }{\sum_{ x' \in {\cal X}} f(x') },
\end{align}
where $f$ is a non-negative likelihood function of data $x$, and ${\cal X}$ is the set of all data.
This general form includes the {\it word2vec} model (Eq.~\eqref{eq:cond_prob_w2v}), where $f(x) = \exp(x)$ and $x = \vect{u}_j \cdot \vect{v}_{i}$.
NCE fits the probability model using a binary classification task in the same way as in negative sampling but using a Bayesian logistic regression.
Specifically, before the training, we know that $1$ in $1+k$ words is sampled from the given data, which can be modeled as prior probabilities
\begin{align}
	\label{eq:prior}
	P(Y_{j} = 1) = \frac{1}{k + 1},\quad P(Y_{j} = 0) = \frac{k}{k + 1}.
\end{align}
Using the Bayes rule, the posterior probability for $Y_{j}$ given word $j$ is given by
\begin{align}
	\label{eq:posterior}
	P\left(Y_{j} \vert j\right) = \frac{
		P\left(j \vert Y_{j}\right)P(Y_{j})
	}{
		P\left(j \vert Y_{j} = 0\right)P(Y_{j} = 0)
		+ P\left(j \vert Y_{j} = 1\right)P(Y_{j} = 1)
	}.
\end{align}
Bearing in mind that word $j$ is sampled from the given data if $Y_{j}=1$ and from the noise distribution $p_0$ if $Y_j = 0$.
Assuming that the given data is generated from the probability model to fit, the class-conditional probability, $P\left(j \vert Y_{j}\right)$, is given by
\begin{align}
	\label{eq:class-cond}
	P(j \vert Y_{j} = 1) & = P_m(\vect{u}_j \cdot \vect{v}_{i}),
	\quad
	P(j \vert Y_{j} = 0)  = p_0 (j).
\end{align}
Putting Eqs.~\eqref{eq:prior}, \eqref{eq:posterior} and \eqref{eq:class-cond} together, the posterior probability for $Y_j$ is given by
\begin{align}
	\label{eq:nce}
	P\left(Y_{j} = 1 \vert j\right) & =
	\frac{
		P_m(\vect{u}_j \cdot \vect{v}_{i}) / (k + 1)
	}{
		P_m(\vect{u}_j \cdot \vect{v}_{i}) / (k + 1)  + kp_0(j) / (k + 1)
	}                                          \\
	                                & = \frac{
		P_m(\vect{u}_j \cdot \vect{v}_{i})
	}{
		P_m(\vect{u}_j \cdot \vect{v}_{i})  + kp_0(j)
	},
\end{align}
which can be rewritten in form of sigmoid function:
\begin{align}
	\label{eq:nce}
	P^{\text{NCE}}\left(Y_{j}=1 \vert j\right) =
	 & = \frac{
		1
	}{
		1 + kp_0(j) / P_m(\vect{u}_j \cdot \vect{v}_{i})
	}           \\
	 & = \frac{
		1
	}{
		1 + \exp\left[ \ln kp_0(j) - \ln P_m(\vect{u}_j \cdot \vect{v}_{i}) \right]
	}           \\
	 & = \frac{
		1
	}{
		1 + \exp\left[ - \ln f(\vect{u}_j \cdot \vect{v}_{i})  + \ln p_0(j) + c \right]
	},
\end{align}
where $c = \ln k + \ln\sum_{ x' \in {\cal X}} f(x') $ is a constant.
NCE maximizes the log-likelihood
\begin{align}
	\label{eq:log-likelihood-logistic-regress}
	{\cal J}^{\text{NCE}} = \sum_{i \in {\cal A}}\sum_{j \in {\cal D}} \left[ Y_{j} \log P^{\text{NCE}}(Y_{j} = 1\vert j) + (1-Y_{j}) \log P^{\text{NCE}}(Y_{j} = 0 \vert j)\right],
\end{align}
by calculating the gradients of ${\cal J}^{\text{NCE}}$ with respect to $\vect{u}_j$, $\vect{v}_{i}$ and iteratively updating them.
An important consequence of this framework is that NCE is an unbiased estimator that has convergence to the optimal embedding in terms of the original {\it word2vec}'s objective function, ${\cal J}$ if we increase the number of words to sample and the training iterations~\autocite{Chia2010,Dyer2014}.

Let us revisit negative sampling from the perspective of NCE.
We rewrite the logistic regression model in negative sampling (Eq.~\eqref{eq:logistic-regress}) in form of the posterior probability:
\begin{align}
	P^{\text{NS}}\left(Y_{j}=1 \vert j\right) & = \frac{
		1
	}{
		1 + \exp\left[ - \left( \vect{u}_j \cdot \vect{v}_{i} + \ln p_0(j) + c \right) + \ln p_0(j) + c \right]
	}                                                    \\
	                                          & = \frac{
		1
	}{
		1 + \exp\left[ - \ln f(\vect{u}_j \cdot \vect{v}_{i}) + \ln p_0(j) + c \right]
	},
\end{align}
where we define the likelihood function $f$ by
\begin{align}
	f(\vect{u}_j \cdot \vect{v}_{i}) = \exp\left( \vect{u}_j \cdot \vect{v}_{i} + \ln p_0(j) + c\right),
\end{align}
which is the unbiased estimator for the probability model
\begin{align}
	\label{eq:unbiasef}
	P_m^{\text{NS}}(\vect{u}_j \cdot \vect{v}_{i}) & = \frac{ f(\vect{u}_j \cdot \vect{v}_{i}) }{\sum_{ j' \in {\cal A}} f(\vect{u}_{j'} \cdot \vect{v}_{i}) },                                                                                               \\
	                                               & = \frac{  p_0(j)\exp( \vect{u}_j \cdot \vect{v}_{i} ) }{\sum_{j' \in \mathcal{A}}  p_0(j') \exp(\vect{u}_{j'} \cdot \vect{v}_{i})},                                                                      \\
	                                               & = \frac{  P^\gamma(j)\exp( \vect{u}_j \cdot \vect{v}_{i} ) }{\sum_{j' \in \mathcal{A}}  P^\gamma(j') \exp(\vect{u}_{j'} \cdot \vect{v}_{i})} \hspace{0.3cm}(\because p_0(\ell) \propto P^\gamma (\ell)).
\end{align}
Taken together, the conditional probability that SGNS \textit{word2vec} actually optimizes is
\begin{align}
	P^{\text{NS}}(j\given i) = P_m^{\text{NS}}(\vect{u}_j \cdot \vect{v}_{i})  =  \frac{  P^\gamma(j)\exp( \vect{u}_j \cdot \vect{v}_{i} ) }{Z'_i},\
\end{align}
where $Z'_i=\sum_{j' \in \mathcal{A}}  P^\gamma(j') \exp(\vect{u}_{j'} \cdot \vect{v}_{i})$.

\subsection{\textit{word2vec} and the gravity model}


Taking into account the implicit bias in the negative sampling  and setting $w=1$, \textit{word2vec} predicts the flow between locations with
\begin{align}
	\widehat{T}_{ij} =  N(i) P(j\given i) = \frac{N(i) P(j)^\gamma \exp(\vect{u}_j \cdot \vect{v}_{i})}{Z'_i}. \label{eq:prob_w2v_ng0}
\end{align}
where $N(i) = N P(i)$ is the frequency of location $i$ in all trajectories, and $N$ is the sum of the frequencies of all locations.
Parameter $\gamma=1$ is a special choice that ensures that, when the embedding dimension is sufficiently large, there exists optimial in-vectors and out-vectors such that $\vect{v}_i = \vect{u}_i$ \autocite{levy2014neural}.
Setting $\gamma = 1$ and substituting $\vect{v}_i = \vect{u}_i$ and $N(i) = N P(i)$ into Eq.~\eqref{eq:prob_w2v_ng0}, the flow predicted by {\it word2vec} is given by
\begin{align}
	\widehat{T}_{ij} =  N \frac{P(i)P(j)\exp(\vect{v}_j \cdot \vect{v}_{i})}{Z'_i}. \label{eq:flow_w2v_ng1}
\end{align}

The flow $\widehat{T}_{ij}$ is symmetric (\ie $\widehat{T}_{ij}=\widehat{T}_{ji}$) because the skip-gram model neglects whether the context $j$ appears before or after the target $i$ in the trajectory.
If we swap $i$ and $j$ in Eq.~\eqref{eq:flow_w2v_ng1}, the numerator remains the same but the denominator can be different.
Therefore, to ensure $\widehat{T}_{ij}=\widehat{T}_{ji}$, the denominator $Z_i$ should be a constant.

Taken together, the {\it word2vec} model with negative sampling predicts a flow in the same form as in the gravity model:
\begin{align}
	\widehat{T}_{ij} \propto  P(i) P(j)  \exp(\vect{v}_j \cdot \vect{v}_{i}), \label{eq:flow_w2v_ng0}
\end{align}

In other words, \emph{word2vec with skip-gram negative sampling is equivalent to the gravity model}, with the mass given by the location's frequency $P(i)$, and the distance measured by their dot similarities.
While the gravity model predicts mobility flows from the given mass and locations, {\it word2vec} finds the mass and locations that best explain the given mobility flow.
Therefore, the embedding distance is inherently tied to mobility flow, and hence, has greater predictive power than the geographic distance.

In practice, because we only have limited amounts of noisy data and the optimization may not find the true optimum, the mathematical result may only approximately hold.
Indeed, we find that the in- and out-vectors tend to be different and that the cosine similarity tends to better capture real-world mobility than the dot similarity.
Nevertheless, a model with dot similarity is still the second-best after cosine similarity (Tables~\ref{supp:table:r2_table},~\ref{supp:table:rmse_exp_table},~\ref{supp:table:rmse_power_table}), and the embedding distance still outperforms all alternatives we considered.

\subsection{Changes to the manuscript}
To reflect our new findings, we have made the following changes to the manuscript:
\begin{itemize}
	\itemsep0em
	\item Following results demonstrating the performance of the embedding distance compared to geographic distance, we add the section \textbf{word2vec and the gravity model}, which summarizes the link between the two models.
	\item We also added the section \textbf{An implicit bias in the negative sampling} to the methods section of our manuscript.
	\item Since the formulation of the gravity used in the above derivations assumes $\gamma = 1$, we use $\gamma = 1$ for all reported evaluation results. No findings have substantively changed.
	\item We added a sentence to the paper speculating on why cosine similarity works best.
	\item We also added a sentence noting the in-vector and out-vector issue, and speculating on reasons/consequences.
\end{itemize}


%
% SECTION-PERFORMANCE COMPARISONS
%
\section{Performance of \textit{word2vec} compared with baselines}
\subsection{Alternatives considered}
In our original manuscript, we compared the performance of what we call the embedding distance---the cosine distance between vectors learned by \textit{word2vec}---to a range of alternatives.
These alternatives included the geographic distance between locations, the distance most commonly used in definitions of the gravity model of mobility, along with the dot product similarity between embedding vectors, and the cosine distance and Jensen-Shannon divergence between vectors obtained from a personalized-page-ranking procedure on the underlying mobility network. Following the advice of the referees, we also add additional baselines to illustrate how the embedding distance performs compared to simpler embedding alternatives and also to a more advanced approach as follows:

\vspace{-0.5cm}
\paragraph{SVD on co-occurrence matrix:}The first of these baselines uses the distance between vectors in an embedding space produced through the singular value decomposition (SVD) of the underlying location co-occurrence matrix. First, we construct the co-occurrence matrix of $N$ organizations, $A_{ij}$ given by the co-occurrence of organizations $i$ and $j$ in the same affiliation trajectory.
Then, we apply the truncated SVD with $d=300$ on the flow matrix $A$ directly.

\vspace{-0.3cm}
\paragraph{Laplacian Eigenmap:} As the second baseline, we consider Laplacian Eigenmap embeddings~\autocite{belkin2003laplacian}, which is one of the most fundamental approaches for graph embedding. We generate Laplacian Eigenmap embeddings by applying a truncated singular value decomposition to matrix $L$ given by $L=D - A$, where
$A$ is the co-occurrence matrix of $N$ organizations with element $A_{ij}$ indicating the number of co-occurrences of organizations $i$ and $j$ in the same affiliation trajectory, and $D$ is the diagonal matrix with diagonals $D_{ii}=\sum_j {A_{ij}}$. We set the dimension to $d=300$.

\vspace{-0.3cm}
\paragraph{Levy's  symmetric SVD \textit{word2vec} distance:} In addition to these suggested baselines, we also consider distances derived from embeddings learned through matrix factorization following the procedure outlined by Levy \& Goldberg~\autocite{levy2014neural}.
They utilize SVD to train the {\it word2vec}'s model observing that training \textit{word2vec} can be understood as a factorization of a matrix.
Here, we compare the performance of \textit{word2vec} embeddings against those calculated directly using Levy's SVD.
First, we construct the matrix of $N$ organizations
\begin{equation}
	\label{eq:levyword2vec}
	M_{ij} = \log\left(\frac{N(i,j) |D|}{N(i) N(j)}\right) - \log(k),
\end{equation}
where $N(i,j)$ is the number of times the location pair $(i,j)$ appears given the window size $w$ in the total corpus $D$, $N(i)=\sum_{j=1}^N N(i,j)$ as the number of items $i$ occurred given the window size $w$ in $D$, and $k$ is the number of negative samples. We used $w=1$ and $k=5$ which is the same setting as in our main result. Then, we factorized matrix $M$  into $V_d \Sigma_d U_d$ using truncated SVD with $d=300$, and used $V_d\sqrt{\Sigma_d}$ as the in-vectors.

\vspace{-0.3cm}
\paragraph{Direct optimization of gravity model:} Finally, we create an embedding by optimizing for the gravity model directly.
Specifically, we construct a gravity matrix of $N$ organizations, where each cell of the matrix is calculated as $T_{ij}/m_i m_j$, where $T_{ij}$ is the number of co-occurrences of organizations $i$ and $j$ in the same affiliation trajectory, and $m_i$ and $m_j$ are the organizations' populations, defined here as the mean annual number of unique mobile and non-mobile authors affiliated. We then embed the gravity matrix using two approaches: the truncated SVD, and multidimensional scaling (MDS), which embeds each location in an N-dimensional space such that pairwise distances are preserved as well as possible.
Typically, MDS uses Euclidean distance to measure the distance between vectors.

\subsection{New experimental parameters}
In addition to the new baselines, we have also incorporated additional experimental parameters into our analysis.
The first is $\gamma$, a hyper-parameter of the \textit{word2vec} model, which defines the exponent which shapes the negative sampling distribution.
The default of \texttt{gensim} implementation of \textit{word2vec} is set to $\gamma = 0.75$, and this was the value used in our original manuscript.
However, to connect \textit{word2vec} to the gravity model of mobility, we assume $\gamma = 1.0$ (see Section 1).
We now report all main results using $\gamma = 1.0$ and also compare its performance to embeddings learned using $\gamma = 0.75$, as we do with the number of embedded dimensions and window size.
We plot the performance of each combination of hyperparameters by their ability to explain real-world flux of scientific mobility (Fig.~\ref{fig:supp:hyperparameter}), and observe no noticeable difference in performance between values of $\gamma = 0.75$ and $\gamma = 1.0$.
Figures in the updated manuscript have been re-drawn with $\gamma = 1.0$, and the findings remain qualitatively similar.
As a side benefit, including the hyperparameter of $\gamma$ also effectively doubles the number of embeddings trained, and that results are so similar across different embedding provides further evidence of the robustness of the approach applied to our data.

%
% Figure - Hyperparameter performance
%
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.8\textwidth]{\figdir/Descriptive/hyperparameter_performance.pdf}
	\caption{
		\textbf{Larger dimensions, smaller window size improves embedding performance.}
		The correlation, or amount of flux explained by the embedding distance with varying skip-gram negative sampling hyperparameters.
		Window size refers to $w$, the size of the context window that defines the context in a trajectory.
		Smaller window sizes result in an embedding that explains more flux.
		Embedding dimensions refer to the size of the embedding vector.
		Larger vectors perform better, though with little difference between 200 and 300.
		Gamma refers to the $\gamma$ parameter in \textit{word2vec}, which shapes the negative sampling distribution.
		A value of $\gamma = 0.75$ is the default for \textit{word2vec}.
		There is virtually no difference in performance for different $\gamma$ values.
		All variants perform better on same-country organization pairs, and worse on different-country pairs, than on all pairs of organizations.
		Embeddings with larger dimensions outperform mid-size embeddings for the different-country case.
	}

	\label{fig:supp:hyperparameter}
\end{figure}

We also consider an experimental parameter differentiating between definitions of \textit{population}, in the context of scientific mobility.
Our original manuscript defined the population of an organization as its average annual number of unique mobile and non-mobile authors who publish with that organizational affiliation.
However, because non-mobile authors do not have mobility trajectories, and are thus not included in the training of the embedding, this definition of population introduces additional, potentially confounding information into the experiment.
To tease apart the impact of population definitions, we consider two additional definitions.
The first defines an organization's population as the average annual number of unique mobile authors, only, who publish with the organization.
The second definition defines the population akin to language, as simply its frequency across all mobility trajectories.

\subsection{Result}

%
% Figure---Metric performance
%
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.95\textwidth]{\figdir/Descriptive/distance_metric_performance.pdf}
	\mycaption
	{Neural embeddings outperform baselines for scientific mobility.}
	{
		Cosine distance between embedding vectors generated with \textit{word2vec} explains more of the flux, and better predicts flux when used in the gravity model of mobility than geographic distance and other baselines.
		Shown is the correlation between the flux and embedding distances, measured with $R^{2}$ (left), and the prediction error when using the distance as input to the gravity model of mobility.
		The asterisk denotes the top-performing metric.
		For prediction error, we show results based on both the exponential and power-law forms of the gravity model.
		All embedding-based methods use dimensions of 300.
		Here, embedding distance is obtained from neural embeddings learned with a window size of 1 and $\gamma = 1$.
		In all cases, organization population is defined as the mean annualized number of unique mobile and non-mobile authors, and flux is calculated for all global mobility.
		Baselines include the top-performing distance metrics calculated between vectors obtained by personalized-page rank (PPR), singular value decomposition (SVD), Laplacian eigenmap, direct-factorization following Levy's approach~\autocite{levy2014neural}, and direct optimization of the gravity model using SVD and multidimensional scaling (MDS), as well as the geographic distance between organizations.
		Embedding distance better explains and predicts flux than any other baseline, though there is some variation by experimental parameters (Table~\ref{supp:table:r2_table}, Table~\ref{supp:table:rmse_exp_table}, and Table~\ref{supp:table:rmse_power_table}).
	}
	\label{fig:supp:distancemetrics}
\end{figure}



We observe that the cosine distance between embedding vectors, which we call here the ``Embedding Distance'', explains more of the flux, and results in lower prediction error than all baselines we considered (See Fig.~\ref{fig:supp:distancemetrics}).
This performance advantage holds across \emph{the exponential and the power-law forms of the gravity model of mobility}.
Although the Euclidean distance between directly-factorized vectors performs comparably, and even narrowly beats the embedding distance under certain experimental settings (Tables~\ref{supp:table:r2_table},~\ref{supp:table:rmse_exp_table},~\ref{supp:table:rmse_power_table}); our approach shows the most consistent and robust performance, outperforming even the direct optimization of the gravity models.


\subsection{Why do neural embeddings outperform matrix factorization?}
Why do neural embedding approaches outperform Levy's {\it word2vec} based on matrix factorization?
%Why do neural-embedding approaches, which rely on stochastic gradient descent, outperform direct matrix factorization, especially given that \textit{word2vec} \emph{is} implicitly approximating factorization?
We find that Levy's {\it word2vec} suffers from extreme values in the matrix to factorize, which are numerous when window size is small.
Remind that Levy's {\it word2vec} generates embeddings by factorizing matrix $M$ with entries
$M_{ij} = \log\left[N(i,j)|D|/ (N(i) N(j))\right] - \log k$ based on the SVD (Eq.~\eqref{eq:levyword2vec}), where
$N(i,j)$ is the number of co-occurrences of organizations $i$ and $j$ in the context window.
An issue is that when $N(i,j)$ is closer to zero, $M_{ij}$ goes to $-\infty$.
To prevent such extreme values, a constant is added to $N(i,j)$ if $N(i,j)=0$ in Levy's {\it word2vec}~\autocite{levy2014neural, Qui2018}.
However, this changes the data to which we fit the model, and small $N(i,j)$ values, which are likely to arise due to noise in data, still have a profound impact on the embedding.
The problem is aggravated when the size, $w$, of context window is small.
As the window size becomes smaller, two institutions are less likely to co-appear in the same window, yielding more affiliation pairs with $N(i,j)=0$, which is the case in our experiment (i.e., we set the context window size $w=1$).
%This is particularly problematic when the context window is small and most affiliation pairs would have no direct flow, leading to $N(i,j)=0$ for most affiliation pairs $(i,j)$.
Also, it is well known that SVD itself is vulnerable to outliers \autocite{xu2012robust, huber1981robust, xu1995robust, chandrasekaran2011rank, candes2011robust}.
The stochastic gradient descent algorithm (SGD), which is employed in neural embedding approaches, is more robust than SVD and can enhance the generalization and effectiveness of {\it word2vec}~\autocite{ma2018power, smith2020generalization, zhang2019algorithmic}.


\subsection{Changes to the manuscript}
To reflect these additional experiments, we have made the following changes to our manuscript:
\begin{itemize}
	\item Re-drawn figures with embeddings using $\gamma = 1.0$, and included performance differences in Fig.~\ref{fig:supp:hyperparameter}, which was added to the supporting materials.
	\item Added Fig~\ref{fig:supp:distancemetrics} and Tables~\ref{supp:table:r2_table},~\ref{supp:table:rmse_exp_table},~\ref{supp:table:rmse_power_table} to the manuscript.
	\item Added sentence speculating on SGD performance differences into supporting materials
\end{itemize}



%
% SECTION- POINT-BY-POINT DISCUSSION
%
\clearpage
\section{Point-by-point response}
All of the referees presented us with excellent, insightful comments and we feel that the changes implemented as a consequence have signiﬁcantly strengthened the manuscript. Here we offer further discussion, and introduce several results and figures to support certain arguments.
\subsection{Response to Referee 1}
\setcounter{section}{1}


\rcomment{%
	To the best of my knowledge, this is the first work providing a thorough analysis of the structure encoded by latent mobility embeddings (e.g., the prestige hierarchy), and this is the first work I am aware of to propose embeddings as an explanatory/exploratory tool in this context. In this way, the work has some originality and significance.

	However, the novelty of this work is quite narrow, as there are several prior works that develop trajectory-based embeddings of human mobility based on the word2vec algorithm. For example, the following references all propose word2vec-inspired embeddings of human mobility data:

	\begin{itemize}
		\item Liu, Xin, Yong Liu, and Xiaoli Li. ``Exploring the Context of Locations for Personalized Location Recommendations.'' IJCAI. 2016.
		\item Feng, Shanshan, et al. ``POI2Vec: geographical latent representation for predicting future visitors.'' AAAI. 2017.
		\item Yao, Zijun, et al. ``Representing Urban Functions through Zone Embedding with Human Mobility Patterns.'' IJCAI. 2018.
		\item Cao, Hancheng, et al. ``Habit2vec: Trajectory semantic embedding for living pattern recognition in population.'' IEEE Transactions on Mobile Computing 19.5 (2019): 1096-1108.
	\end{itemize}

	Notably, the location embedding methodology in Liu et al (2016) (see Section 3.1) is essentially the same as the approach used in the submitted manuscript, and the work by Liu et al (2016) is well-known with over 100 citations.
}



\response{Thank you for pointing out the missing references!
	We have carefully read through each, and learned a lot in the process.
	They have now been incorporated into our manuscript's introduction.
	Although there are existing works on trajectory-based mobility embedding, we argue that our new result establishing the mathematical equivalence of \textit{word2vec} and the gravity model, together with our extensive evaluation of baselines, represents a novel and substantial contribution.
}

\rcomment{
	I found the specific use of the SGNS algorithm unmotivated. There are many ways to generate useful low-dimensional embeddings, so why is the SGNS method prioritized above others? This method is popular, but popularity alone is not sufficient justification and I see no theoretical reason why it should be the most appropriate method. For example, a key point in this manuscript is the fact that the the SGNS embeddings capture the flux between locations better than geographic distance. However, if this is such an important point, then why not simply learn a low-dimensional embedding approximation of the flux metric directly? such as SVD?
}

\response{
	Thank you for the suggestion!
	Since our original submission, we have uncovered a mathematical connection between the skip-gram model with negative sampling, and the gravity model of mobility.
	Namely, we find that \emph{SGNS is a gravity model}.
	This finding explains why SGNS performs so well and demonstrates the utility of the model for studying contexts where gravity relationships are expected, such as in human mobility.
	We refer you to Section 1 of this letter, above, for an investigation of this connection, which we believe to be not only a justification of our choice of the \textit{word2vec} model to study mobility, but also a major contribution to the understanding of neural embeddings and the gravity model themselves.

	Additionally, we compared the performance of SGNS to several baselines, including the distances between vectors obtained with the singular-value decomposition, the Laplacian eigenmap, the Levy
	\& Goldberg's~\autocite{levy2014neural} matrix factorization \textit{word2vec}, and the directly optimization of the gravity model with singular-value decomposition and multidimensional scaling.
	We find that in nearly every setting, SGNS outperforms all of these baselines, further illustrating the potential of such models for embedding and studying mobility.
	We refer you to Section 2 of this rebuttal letter, above, for more detailed results and the changes we made to our manuscript.
}



\rcomment{%
	The methodology has potential limitations in terms of reproducibility and robustness, which should be addressed. This is no specific fault of the authors, but both the SGNS/word2vec algorithm and the UMAP algorithm are inherently stochastic. This means that these approaches involve inherent randomness, and for researchers to replicate the exact effect seen in this manuscript, one would need to ensure the exact same random seeds etc. were used. In my view, this is a serious limitation, especially given that the UMAP results and learned embeddings are reified and treated as first-class objects of scientific study.
}

\response{Thank you for this comment. While robustness is a general concern for these methods, we found that their application to our datasets were in fact quite robust, and observed no evidence that findings were the result of methodological artifacts. This is confirmed in extra work done in response to these reviews.

	We re-ran all of our results using different starting seeds and found all results were qualitatively similar, and almost quantitatively identical.
	In addition, we expanded the set of experimental parameters, training 18 different embeddings with different dimensions, window sizes, and now, $\gamma$.
	We found that while performance varies across each combination of parameters, the main results remain qualitatively similar.
	While we can run more combinations of parameters, and more iterations of \textit{word2vec}, we see no evidence that results will drastically vary.

	We believe that the ratio of the number of organizations represented in our dataset to the total number of mobility trajectories makes our results especially robust.
	About 8,661 organizations are represented across over three million career trajectories of scientists, such that nearly every organization appears enough times, and in enough contexts, to produce a robust vector representation.
	However, this may not hold true for all problem domains—for example, natural language includes many millions of tokens, many of which will occur only a few times, allowing stochasticity to have a much stronger effect on the final representation.
	Heeding this as a general concern, we have added the following sentence to our limitations paragraph in our conclusion:

	“We note that all results shown here are robust to this stochasticity, likely emerging from the limited ``vocabulary'' of scientific mobility, airports, and accommodations (several thousand) and the massive datasets used (several million trajectories). Applications of \textit{word2vec} to problem domains where the ratio of the vocabulary to data is smaller, however, should be implemented with caution to ensure that findings are not the result of random fluctuations.”

}


\rcomment{%
	Another interesting point in this work is that the authors chose to not report p-values or the results of statistical significance tests (except confidence intervals). I presume this was a conscious decision, which I do not necessarily agree with. I see no critical error in the approach used by the authors, but I would have found aspects of the manuscript easier to understand in places with results of statistical tests (e.g., associated with goodness-of-fit tests formally test that the embedding-based distances are more correlated with the flux values). Given the sample sizes involved, I have no doubt that the results are statistically significant. However, I personally do see value in rigorously performed frequentist hypothesis testing, and I felt that this manuscript could be improved by employing such techniques.

	Lastly, the authors do not actually state how the 99 confidence intervals were constructed. (E.g., was this via a bootstrap approach?) This detail should be added.
}

\response{Thank you for this feedback.
	You are correct that it was a conscious choice to not report statistical significance in the form of a $p$-value.
	The use of statistical significance is contentious, and Nature itself has hosted some of this debate~\autocite{amrhein2019statistics}.
	As such, we avoided $p$-values in our research when it made sense to do so.
	In our analysis, all significance levels were below 0.0001, with most computationally indistinguishable from zero.
	However, we recognize that $p$-values can help readers interpret results.
	As such, we have opted to add to a sentence such as \say{\textit{Correlation is calculated on the data in the log-log scale ($p < 0.0001$ across all fits)}.} or similar to relevant figures in the main text and supporting materials.

	We have also provided information on how confidence intervals were calculated in relevant figure captions.
	For example, we state in figure 2's caption that \say{\textit{99\% confidence intervals are plotted for the mean flux in each bin based on a normal distribution.}}, which should give readers enough information to reproduce the confidence intervals following their standard definition.
	Other confidence intervals, such as intervals for the loess regression line in figure 6, are calculated using the smoothing functions in the \texttt{ggplot2} package of R and we refer readers to the relevant documentation.
}


\rcomment{%
	F. Suggested improvements:
	References and discussion of prior work using word2vec-style embeddings of mobility data.
	Additional experiments using alternative embedding methods, especially an SVD-based embedding method, as well as robustness/reproducibility studies across different runs of the SGNS would be essential to strengthen the paper.
	In addition, some additional results demonstrating how the embeddings provide novel scientific insight, rather than just recapitulating known structures, would be essential to make this work above-the-bar for a high-impact publication such as Nature.

}


\response{We believe that our new mathematical results substantially improve the manuscript, extends its contribution, demonstrates novel scientific insight, and justifies the use of \textit{word2vec} towards mobility data. By adding this, and addressing your other points (above), we believe that the revised manuscript sufficiently remedies your concerns.

}


%===================================================

%
% Response to reviewer 2
%
\clearpage
\setcounter{section}{3}
\subsection{Response to Referee 2}
\setcounter{section}{2}


\rcomment{%
	Although the paper is well written and explores a very interesting problem, I have certain criticism for the used methodology. After obtaining a representation of each dataset, the authors defined an embedding distance as the cosine distance between vectors in the space, while defining distance as the geographical distance in the original, spatially embedded datasets. Subsequently they used the Gravity Law to estimate the flux between location pairs of vector pairs in the spatial and embedded space respectively. The Gravity Law, applied on mobility, assumes that the number of people moving between two populations is proportional to the multiplicative of the sizes of the populations and a deterrence function. Research during the last decades on transportation, spatial networks and human mobility suggests that this deterrence function may take a functional form of an inverse exponential or an inverse power-law, as it has been acknowledged by the authors. However, the emergence of these different deterrence functions can be due to fundamentally different mechanisms (for a review on the topic see Barthélemy, Physics Reports 499, 1, 2011, a paper not cited in the manuscript). For example, the different cost functions (linear or logarithmic) to create link at a given distance, the spatial granularity of the observation, or even the heterogeneity of the moving individuals in terms of various factors, just to mention a few examples. Instead, the authors of the actual paper arbitrarily assumed a power-law deterrence function in the spatial case and an exponential deterrence function in the embedding space without further arguing their choice (other than fitting on the data). Furthermore, assuming different deterrence functions for the flux estimation using Gravity Law makes the estimated fluxes incomparable, resulting the invalidation of results depicted in Figure 2.
}

\response{Thank you for noting this.
	We carefully read out the Barthélemy, Physics Reports 499, 1, 2011, and found it incredibly educational, referencing it in our manuscript.
	Past work has argued that the power-law decay function is appropriate for modelling geographic distance---being more suitable for the analysis of large, complex, and scale-free spatial systems with long-range spatial memory---whereas the exponential gravity model fits well with the simpler system with no spatial memory~\autocite{chen2015distance}, such as might be encoded in the \textit{word2vec} embedding space.
	We report the performance of the flux estimation and prediction using both deterrence functions (see Section 2, above), finding that although the absolute values vary, the qualitative pattern remains the same---the embedding distance outperforms geographic distance regardless of the function used.
	We also elaborate on the connection between the embedding distance and exponential model in our discussion on the relationship between \textit{word2vec} and the gravity model of mobility (see Section 1, above).
	In addition, we add the following line to our manuscript, noting these considerations,

	\medskip
	\say{The decay function $f(r_{ij})$ defines the effect of distance, and so different decay functions can model fundamentally different mechanisms [...] such as the cost functions for a given distance and the spatial granularity of the observation.
		For geographic distance, we define $f(r_{ij})$ as the standard power-law function, and for the embedding distance, we use the exponential function, selected as the best performing for each case though there is no qualitative difference---the embedding distance outperforms geographic distance regardless of the type of gravity model used (see Supporting Information).}

}





\rcomment{
	I would suggest to the authors to carefully cite other papers applying Word2Vec embedding on spatial data, especially focusing on location embedding based on trajectory sequences. Examples are:
	\begin{itemize}
		\item A. Solomon et al., Predict Demographic Information Using Word2Vec on Spatial Trajectories., ACM UMAP '18: Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization, July 2018 Pages 331–339, https://doi.org/10.1145/3209219.3209224
		\item A. Crivellari and E. Beinat, From Motion Activity to Geo-Embeddings: Generating and Exploring Vector Representations of Locations, Traces and Visitors through Large-Scale Mobility Data., ISPRS Int. J. Geo-Inf. 2019, 8(3), 134 (2019); https://doi.org/10.3390/ijgi8030134

	\end{itemize}

	The application of the Word2Vec method on the studied trajectory sequences is not well explained. Without explaining the embedding method briefly in the main text the reader has no idea how to interpret the embedding distance, which is in the hear of the study.

}

\response{Thank you.
	We have added the following sentence to the introduction: “It works under the notion that a good representation should facilitate prediction, and so learns a mapping of target locations (words) that best predict locations that co-appear in the same trajectory (sentence).”

	In addition, we provided more explicit mathematical detail of word2vec in the new subsection entitled “word2vec and the gravity model” (see Section 1, above).

}

\rcomment{

	The original SkipGram method expects environments of locations (originally words) with fixed length, however this is not explained in the manuscript nor in Methods or the SI.
	Related question to the previous one. Let’s assume that the authors used fixed window lengths and, as mentioned in Methods Embedding section (and demonstrate in Fig.S4 - not referenced in Methods), that the windows size w=1 gives the best correlations in case of scientific mobility. This might be true in absolute terms but it is an unfair comparison to larger window sizes, as there are way more short trajectories in the data than longer ones (see Fig.S3), thus the flux prediction of w=1 is based on a way larger sample than for w$>$1. Undersampling the ensemble of trajectories used for w=1 predictions might provide a fair way of comparing results of different window sizes.
}

\response{
	Thank you for this comment.
	We do not, however, believe that this is a problem in our analysis.
	Raw co-occurrence, which is used to define our ``flux'' between organizations, is always calculated using all organizations in a trajectory, and so window size does not affect flux.
	Window size does affect \textit{word2vec} and setting a smaller/larger window size should not make results incomparable.
	When we set $w = 1$, \textit{word2vec} will generate training pairs at most 1 position away from each target location, for all possible targets.
	When we instead set $w = 2$, \textit{word2vec} will generate training pairs the same way, but at most 2 positions away from each target---this means that the data for $w = 2$ will be \textit{larger}, containing all training pairs used for $w = 1$ as a subset.
	The same follows for all $w > 1$, each generating a larger and larger set of training examples up until $w$ is larger than the length of the longest trajectory.

	What is notable about our findings, therefore, is that even though a model trained with $w = 1$ generates fewer training samples, it still outperforms settings with $w > 1$, which use more training samples, and which should capture higher-order structures in the data.
	We speculate that this is because the word2vec model with $w = 1$ is equivalent to the gravity model of mobility (see Section 1).
}

\rcomment{
	The other very important parameter of the SkipGram method is the embedding dimension, which is explained in the Methods but it is not explored. E.g. to choose the ideal dimension for embedding one may sweep through a broad range of possible dimensions and not only explore three values as described in the SI (Fig.S4).
	Their main observations in section “Embeddings provide functional distance between locations” is that “…our results demonstrate that, consistently, the embedding distance better captures patterns of actual mobility than does the geographic distance.”. This is a great verification of their method but it is somewhat expected from an embedding, which is built on mobility trajectories between places, as compared to cases based on simply geographic distance.

}
\response{
	Thank you for these comments.
	First, we added a line to the methods section detailing the \textit{word2vec} we used, and noting that these achieved the best performance: \say{\textit{For main results regarding scientific mobility, we used $d=300$ and $w=1$, which were the parameters that best explained the flux between locations, though results were robust across different settings (Fig....)}}.

	A full search of the hyper-parameter space would certainly be interesting and useful, however we believe that set of hyper-parameters we considered here is sufficient to establish the utility of the approach, and demonstrate that performance is only slightly different based on different hyper-parameters.
	There are previous studies on the optimal dimensionality of word embedding \autocite{yin2018understand, yin2018dimensionality}. Also,
	we note that in previous work from our group, we explored the optimal dimensions for graph embedding~\autocite{gu2020dimensions}, finding that while a sufficient number of dimensions is necessary to capture the structure, increasing the dimensions does not heavily deteriorate the performance, a property common across deep learning methods.
	Given that there is little performance difference between $d = 100$ and $d = 300$, we opt to use the larger embeddings at $d = 300$, which are large enough to capture the complexity of mobility, yet small enough to allow an efficient computational workflow.
}

\rcomment{
	When datasets in the intro are explained, it should be mentioned that the detailed descriptions are in the Methods section.
}
\response{Thank you. We have added a parenthetical to the introduction after stating the datasets, as follows:  \say{We embed trajectories from three massive datasets: U.S. passenger flight itinerary records, Korean accommodation reservations, and a dataset of scientists' career mobility between organizations captured in bibliometric records (Detailed descriptions are available in the Methods).}
}
\rcomment{
	Some minors comments
	\begin{enumerate}
		\item There is no reference to Fig.5e and f in the text.
		\item Second paragraph on page 10: the authors discuss Fig.6 here but missed to refer the figure.
		\item Page 15, last row: double point in the end of sentence.
		\item SI Page 2, S3: “. if” $\rightarrow$ “. If”
	\end{enumerate}
}
\response{Thank you, we have addressed these comments and have updated the manuscript accordingly,
}

\rcomment{
	SI Fig.S1c: Here the authors conclude that the proportions “decreased over time”. These proportions seem to be rather flat without strong decreasing trends. In any case, if the authors would like to indicate minor changes here, it would be helpful for the reader if a grid would be posted on the figure.
}
\response{Thank you, we agree that the trend is flat, rather than decreasing. We have adjusted the text accordingly, and also added a grid to the figure so that it can be read more easily.

}

\rcomment{
	SI Fig.S3: Posted numerical values on the figures and the values described in the caption do not match.
}
\response{Thank you for pointing out this oversight. The caption values have been updated.
}

\rcomment{SI Fig.S6: Please indicate what is RMSE in the caption. The authors defined it in the main text but the SI should be self consistent too.
}
\response{We have added the line “RMSE is the root-mean-squared error between the actual and predicted values.” to Fig. S6 as well as all other SI figures that make use of the same visualization.
}

\rcomment{SI Fig.S7: The authors state that “The gravity model with the exponential decay function outperforms that with a power-decay function.” - this does not seem to be true for the “International” case. Please specify this in the caption.
}

\response{We have updated the relevant line of the caption to read as follows:
	“The gravity model with the exponential decay function slightly outperforms that with a power-decay function except in the case of international-only mobility, for which power-decay performs slightly better.”
}

%
%
% Response to reviewer 3
\clearpage
\setcounter{section}{3}
\subsection{Response to Referee 3}
\setcounter{section}{3}

\rcomment{The analysis does not compare embeddings, which would not be a problem if they were making
	an inference other than we can embed micro-mobility to trace macro-mobility. They use an
	efficient (Google engineers developed it), single-layer embedding from 2013, which produces
	linear associations. But the paper doesn’t take advantage of the revolution in deep neural
	network models that now produce powerful, nonlinear and noncontiguous (e.g., using the
	attention architecture) models that now dominate analysis of language, and are making very
	power inroads in machine vision. To be clear, deeper models require much data and computational power to tune—they are an engineering feat—but if developed in these settings, they would likely dramatically improve the prediction of flux. They take into account directionality (e.g., a person moving from one opportunity to another) and would independently capture people how people distinctly move up and down the hierarchy or from place to place (e.g., a in bi-directional model like BERT). A lack of discussion or demonstration of which model works best--and the unfounded inference that direction does not matter felt weak without a demonstration that they would not have worked better. They did and do work much better for language models and for image generation...why would they not work for analysis of human mobility? (I’m sure they would, but this would time and independent development.)
}

\response{Thank you for this insightful comment!
	We think that there is a great deal of potential in methods like BERT for representing mobility data.
	What initially drew us to \textit{word2vec}, however, was its simplicity, ubiquity, and mathematical tractability (at least compared to other, more sophisticated approaches).
	This last attribute proved very useful, as in exploring the mathematical properties of the model, we discovered a deep connection between \textit{word2vec} and the gravity model of mobility, which explains why it learns gravity-like relationships between entities (see Section 1, this letter).
	We believe that this new discovery, now included in the manuscript, strongly grounds our use of \textit{word2vec} here.

	We agree that the new revolutions in neural embeddings offer huge potential for the study of human mobility, however we argue that our contribution is dependent on \textit{word2vec}, both for its equivelance with the gravity model of mobility, as well as for the model's simplicity and ubiquity.
	\textit{Word2vec} is also efficient; deep neural models often require tremendous amounts of training data ($\sim$3 billion words in BERT) and model parameters, making them infeasible for most studies of mobility.
	We hope that this manuscript serves as a foundation that others can build upon, applying more and more sophisticated neural embedding techniques to gain deeper insights into mobility data.


	To highlight the importance of alternative models, however, we added the following line to our conclusion: \say{\textit{Future studies may also consider bi-directional embeddings, such as BERT~\autocite{devlin2018bert}, to incorporate directionality.}}.

}

\rcomment{
	The paper does not compare human transition (or co-affiliation) data with any other kinds of
	data that could be used for embedding. Does author byline data do better than the embedding
	of collaborating institutions (almost certainly), than co-mention of the institutions in the press, as
	with institutional collaborations (e.g, the Harvard-MIT Broad Institute, or joint university
	conferences; these will be stronger signals, but there are fewer of them), than advisor-student
	university pairs (although many of these will be captured by the authors’ approach when
	students publish at their home institutions before securing jobs elsewhere).
}

\response{
	Thank you for this comment!
	We feel that applying these embeddings to other areas of scientific activity would be incredibly enlightening.
	Your recommendation of collaboration (based on the author byline) and advisor-student pairs are especially interesting, and we intend to approach such questions in future studies.
	However, for this manuscript, we believe that mobility is the most appropriate area of study, due to the importance of mobility to science, economics, and innovation, as well as the fundamental mathematical relationship between \textit{word2vec} and the gravity model of mobility, which we discovered since our first submission.
}


\rcomment{
	There are known concerns with clustering a 300 dimensional objects because of the curse of
	dimensionality. Many have shown that reducing dimensionality with UMAP to a nontrivial
	number of dimensions (e.g., 30), which preserves both local and some global distance, then
	clustering on those dimensions are much more stable (e.g., this post). This was one
	concern with the stability of the clustering analysis


}

\response{Thank you for this suggestion.
	We looked into this, using UMAP to reduce the country-level embeddings from 300 down to 30 dimensions, however, the results were incomprehensible, and much inferior to those using 300 dimensions.
	Many dimensionality reduction techniques reduce the number of dimensions by maintaining pairwise distances between vectors, so that the result can be well-suited for clustering algorithms.
	However, our approach already uses raw pairwise similarities to perform hierarchical clustering, so approximating this distance in 30 dimensions, and then using the new distances to perform clustering is redundant.
	We also find that the curse of dimensionality is not a major issue in our analysis, at least given the size of our data and the number of locations represented---\textit{word2vec} seems able to learn dense vector-space representations.
}

\rcomment{
	P6Para3: “For THEIR purpose…”
}

\response{Thank you, we have corrected this mistake.
}

\rcomment{
	P8Para1: “by explicitly the relative importance of different level of the hierarchy with a scaling parameter r ” which is never described or explained. Considering it was derived by the authors in a very recent paper, no one will know what it is.
}

\response{Thank you, we have edited the text to read as “We quantify the relative importance of geography (by region), and language (by the most widely-spoken language of each country) using the element-centric clustering similarity, a method that can compare hierarchical clustering and disjoint clustering (geography, language...) at the different levels of hierarchy by explicitly adjusting a scaling parameter $r$, acting like a zooming lens”
}


%
% Response to referee 4
%
\clearpage
\setcounter{section}{3}
\subsection{Response to Referee 4}
\setcounter{section}{4}

\rcomment{
	The authors define the distance function in terms of either the geographic distance between locations or their functional distance in the vector space. The increase in R2, as they switch to the embedding distance, is rather convincing. One of the models they use as a reference is the gravity model, but they also mention the radiation model. I am curious if they tried a comparison with that, and what was the outcome. I suspect that the radiation model would have a comparable performance to the gravity one. But as distance is not the only factor there, is a comparison at all feasible?
}

\response{Thank you for this suggestion.
	We agree that comparing the radiation model would be incredibly interesting.
	However, we originally did not use the radiation model because our dataset on scientific mobility lacked clear directionality, which is required by the radiation model of mobility.
	Scientific mobility is incredibly complex, the most common form of mobility being co-affiliation, or a researcher holding multiple affiliations simultaneously.
	In these cases, it is difficult to discern a researcher's main affiliation (where they start), and eventual affiliation (where they end up).
	As such, we conceive of scientific mobility as having no direction (see S2 Text).
	Given these considerations, we instead chose to use the standard gravity model of mobility, which models mobility as symmetric.
	Moreover, we have recently discovered an underlying mathematical relationship between \textit{word2vec} and the gravity model of mobility, further justifying our choice.
	While we are excited to investigate the use of neural embeddings with the radiation model, we believe it to be outside the scope of our current submission.
}

\rcomment{
	My second group of questions pertain to the prestige hierarchy, that really drives the dynamics of scientific mobility. The technical description addressed most of them. However, I was wondering, is it all possible to go beyond the descriptive part, and show cases where the more accurate embedding base distances do make a difference? That is, using them, we do not only obtain better correlations with the ranking data, but also allows us to address problems that we could not without the embedding distance? For example, some results have shown that a paper coauthored by two authors at different high prestige institutions has higher impact, than writing a paper with two authors of the same high prestige institution—would embedding help us pull apart such effects? Here I am not trying to make the authors to write a different paper, so maybe a little forward thinking may be enough, if doing the actual work is too time consuming, and goes beyond the current narrative.
}

\response{Thank you for this idea!
	We believe that this is an interesting topic and we hope to investigate it in the future.
	In your particular example, an embedding distance trained using sequences of co-authorship on the same paper might better capture the \textit{collaborative} distance between institutions.
	Then, using that distance, one could measure the ``embedding distance'' spanned by all authors in the byline and answer the question of whether typical collaborations relate to impact or not.
	This is certainly a topic that  we intend to pursue in a future study.
	In the case of scientific mobility, we might also consider the ``embedding distance'' spanned by an individual's career, and relate this to their demographics and scientific impact.

	However, while these topics are exciting, we believe that they fall outside the scope of the current manuscript, especially given the discoveries we made concerning a fundamental connection between the gravity law of mobility and \textit{word2vec}, which adds a substantial amount of new results to the manuscript.
}


\rcomment{
	Finally, one cannot escape the fact that COVID changes our mobility patterns. Many groups have used mobile phone data to explore these changes. I wonder, how would the pre-covid embeddings apply to post-covid mobility? Once again, I am mindful that this would require access to such data, but I would not be surprised at all if the authors have access to such data—in which case this is remains an intriguing question.
}

\response{This is an excellent idea and one that we have been interested in addressing as well.
	Unfortunately, however, we do not have access to such data, at least not in large enough quantities to be representative.
	Publications are on a time lag, compared to actual events.
	For example, a researcher may write a paper while studying at university $A$, but then before the final paper is published, move to university $B$, so while they now have a $B$ affiliation, in our data all we see is the $A$ one.
	As such, we would not necessarily expect to see the impact of COVID on bibliometric data until later in 2021 or early 2022, when researchers have had the chance to publish with their new institution.
	We also note that some activity will, of course, be visible, especially given the number of researchers rapidly publishing on the subject of COVID-19, however, we consider this to be an unrepresentative sample, given that it is concentrated in a few disciplines.

	In the future, however, we hope to have access to a fully-representative sample of mobility data to study the effects of COVID-19 on global scientific labor flows.
	We can use techniques developed by other researchers, such as those used by Hamilton, Leskovec, \& Jurafsky \autocite{hamilton-etal-2016-diachronic}, to align embeddings trained on different data in order to quantify the changes in global mobility.
	We look forward to approaching this, as well as other exciting projects in the future.
}








%
% R2 Table
%
\clearpage

\begin{landscape}

	\providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
	\providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
	\providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
	\providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

	\begin{table}[ht]
		\caption{
			\textbf{Correlation between flux and distance over metrics, experimental parameters}.
			Each cell corresponds to the correlation between the real-world flux between scientific organizations (measured with $R^{2}$) and baseline metrics, shown by subsets of mobility data and by definitions of organization population.
			The asterisk denotes the top-performing distance metric by column.
			Distance metrics are ordered from highest $R^{2}$ to lowest, based on global mobility with organization population defined using all mobile and non-mobile authors.
			``All'' means that population is defined as the average yearly number of unique mobile and non-mobile scholars who published with the organizations' affiliation;
			population is defined in the same way for ``Mobile only'', except only using unique mobile researchers;
			``Raw freq'' means that organization populations are defined as their frequency across all the trajectories, similar to word frequency in language embedding.
			Embedding distance, measured as the cosine distance between embedding vectors, explains more of the flux than baselines in nearly every case, except using raw frequency population and domestic and international mobility, where direct optimization of the gravity model works better, as well as Levy's factorization~\autocite{levy2014neural} for domestic and international only mobility.
		}
		\label{supp:table:r2_table}
		\begin{centerbox}
			\begin{threeparttable}
				\small
				\setlength{\tabcolsep}{0pt}
				\begin{tabular}{l l l l l l l l l l}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{} \hspace{6pt}\huxbpad{0pt}}                        &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{All} \hspace{6pt}\huxbpad{0pt}}                                          &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Mobile only} \hspace{6pt}\huxbpad{0pt}}                                  &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Raw freq} \hspace{6pt}\huxbpad{0pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{ } \hspace{6pt}\huxbpad{6pt}}                     &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}-}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.481 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.418 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.435 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.492 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.456 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.489 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.325 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.251 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.252 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity MDS euclidean} \hspace{6pt}\huxbpad{6pt}} &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.355 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.165 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.161 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.328 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.112 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.115 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.369 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.174 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.164 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's euclidean} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.341 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.369 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.382 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.213 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.271 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.284 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.305 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.323 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.334 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding dot} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.341 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.313 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.316 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.254 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.265 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.267 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.218 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.181 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.177 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{SVD cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.247 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.297 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.309 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.213 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.314 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.325 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.111 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.152 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.16 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Geographic} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.219 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.174 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.197 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.188 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.157 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.176 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.04 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.019 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.03 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Laplacian cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.212 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.199 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.218 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.176 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.157 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.18 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.079 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.1 \hspace{6pt}\huxbpad{6pt}}                                                    &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.111 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's cosine} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.208 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.227 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.231 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.169 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.246 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.246 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.057 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.054 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.053 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR JSD} \hspace{6pt}\huxbpad{6pt}}               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.194 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.276 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.276 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.218 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.335 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.335 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.012 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.077 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.069 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.138 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.136 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.143 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.196 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.186 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.197 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.13 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.149 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.152 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity SVD cosine} \hspace{6pt}\huxbpad{6pt}}    &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.122 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.118 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.122 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.118 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.133 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.138 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.056 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.047 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.05 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's dot} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.004 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.002 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.002 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.013 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.002 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.004 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.039 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.034 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.035 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}
				\end{tabular}
			\end{threeparttable}\par\end{centerbox}

	\end{table}

\end{landscape}



%
% RMSE TABLE --- Exponential model
%
\clearpage
\begin{landscape}

	\providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
	\providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
	\providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
	\providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}


	\begin{table}[ht]
		\caption{
			\textbf{Prediction error between actual and predicted mobility with exponential gravity model, by metrics and experimental parameters}.
			Each cell corresponds to the prediction error (measured with root mean squared error) when using each distance as input to the exponential form of the gravity model of mobility to predict the flux between organizations, shown by subsets of mobility data, and by definitions of organization population.
			The asterisk denotes the top-performing distance metric by column (lowest prediction error).
			Distance metrics are ordered from lowest prediction error to highest, based on global mobility with organization population defined using all mobile and non-mobile authors.
			``All'' means that population is defined as the average yearly number of unique mobile and non-mobile scholars who published with the organizations' affiliation;
			population is defined in the same way for ``Mobile only'', except only using unique mobile researchers;
			``Raw freq'' means that organization populations are defined as their frequency across all the trajectories, similar to word frequency in language embedding.
			Embedding distance, measured as the cosine distance between embedding vectors, results in better predictions of mobility than baselines in nearly every case, however Levy's factorization~\autocite{levy2014neural} perform better in the case of international and domestic only mobility when using raw frequency populations.
		}
		\label{supp:table:rmse_exp_table}
		\begin{centerbox}
			\begin{threeparttable}
				\small
				\setlength{\tabcolsep}{0pt}
				\begin{tabular}{l l l l l l l l l l}

					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{} \hspace{6pt}\huxbpad{0pt}}                        &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{All} \hspace{6pt}\huxbpad{0pt}}                                          &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Mobile only} \hspace{6pt}\huxbpad{0pt}}                                  &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Raw freq} \hspace{6pt}\huxbpad{0pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{ } \hspace{6pt}\huxbpad{6pt}}                     &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}-}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.713 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.76 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.737 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.702 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.749 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.713 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.715 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.764 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.748 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's euclidean} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.803 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.791 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.771 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.874 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.867 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.844 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.725 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.726 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.705 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding dot} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.803 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.825 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.811 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.851 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.854 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.769 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.798 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.784 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{SVD cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.859 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.835 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.815 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.874 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.841 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.82 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.82 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.812 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.792 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Laplacian cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.878 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.891 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.867 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.894 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.933 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.904 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.835 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.837 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.815 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's cosine} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.881 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.875 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.898 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.882 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.866 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.844 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.858 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.841 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR JSD} \hspace{6pt}\huxbpad{6pt}}               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.888 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.847 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.835 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.871 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.828 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.814 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.865 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.847 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.834 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity MDS euclidean} \hspace{6pt}\huxbpad{6pt}} &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.904 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.944 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.93 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.907 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.972 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.955 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.793 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.838 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.821 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.918 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.925 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.908 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.884 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.916 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.894 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.812 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.814 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.796 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Geographic} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.929 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.951 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.928 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.973 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.002 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.983 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.856 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.875 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.853 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity SVD cosine} \hspace{6pt}\huxbpad{6pt}}    &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.933 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.939 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.923 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.92 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.946 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.927 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.853 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.865 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.847 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's dot} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.987 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.995 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.98 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.979 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.014 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.996 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.853 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.867 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.849 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}
				\end{tabular}
			\end{threeparttable}\par\end{centerbox}

	\end{table}

\end{landscape} % End r2 table page




%
% RMSE TABLE --- Power Law
%
\clearpage
\begin{landscape}

	\providecommand{\huxb}[2]{\arrayrulecolor[RGB]{#1}\global\arrayrulewidth=#2pt}
	\providecommand{\huxvb}[2]{\color[RGB]{#1}\vrule width #2pt}
	\providecommand{\huxtpad}[1]{\rule{0pt}{#1}}
	\providecommand{\huxbpad}[1]{\rule[-#1]{0pt}{#1}}

	\begin{table}[ht]
		\caption{
			\textbf{Prediction error between actual and predicted mobility with power-law gravity model, by metrics and experimental parameters}.
			Each cell corresponds to the prediction error (measured with root mean squared error) when using each distance as input to the power-law form of the gravity model of mobility to predict the flux between organizations, shown by subsets of mobility data, and by definitions of organization population.
			The asterisk denotes the top-performing distance metric by column (lowest prediction error).
			Distance metrics are ordered from lowest prediction error to highest, based on global mobility with organization population defined using all mobile and non-mobile authors.
			``All'' means that population is defined as the average yearly number of unique mobile and non-mobile scholars who published with the organizations' affiliation;
			population is defined in the same way for ``Mobile only'', except only using unique mobile researchers;
			``Raw freq'' means that organization populations are defined as their frequency across all the trajectories, similar to word frequency in language embedding.
			Embedding distance, measured as the cosine distance between embedding vectors, results in better predictions for global mobility with ``All'; and ``Mobile only'' definitions of population, though direct optimization of the gravity model with MDS performs better when using raw frequencies to measure organization population, and Levy's factorization~\autocite{levy2014neural} perform better here with domestic and international only mobility.
		}
		\label{supp:table:rmse_power_table}
		\begin{centerbox}
			\begin{threeparttable}
				\small
				\setlength{\tabcolsep}{0pt}
				\begin{tabular}{l l l l l l l l l l}


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{} \hspace{6pt}\huxbpad{0pt}}                        &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{All} \hspace{6pt}\huxbpad{0pt}}                                          &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Mobile only} \hspace{6pt}\huxbpad{0pt}}                                  &
					\multicolumn{3}{c!{\huxvb{0, 0, 0}{0}}}{\huxtpad{0pt + 1em}\centering \hspace{6pt} \textbf{Raw freq} \hspace{6pt}\huxbpad{0pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{ } \hspace{6pt}\huxbpad{6pt}}                     &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}}                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Global} \hspace{6pt}\huxbpad{6pt}}                                        &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{Domestic} \hspace{6pt}\huxbpad{6pt}}                                      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} \textbf{International} \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}->{\huxb{0, 0, 0}{0.5}}-}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.743 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.784 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.76 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.73 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.784 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.749 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.714 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.762 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.745 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's euclidean} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.78 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.776 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.755 \hspace{6pt}\huxbpad{6pt}}                                               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.844 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.847 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.822 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.703 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.711 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.691 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity MDS euclidean} \hspace{6pt}\huxbpad{6pt}} &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.795 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.91 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.898 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.808 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.957 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.939 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} *0.691 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.802 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.79 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Embedding dot} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.822 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.844 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.831 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.864 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.879 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.863 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.783 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.809 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.795 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{SVD cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.839 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.785 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.761 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.89 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.834 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.81 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.792 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.752 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.728 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Laplacian cosine} \hspace{6pt}\huxbpad{6pt}}      &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.87 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.837 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.801 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.937 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.919 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.886 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.804 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.772 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.735 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Geographic} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.874 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.905 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.879 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.888 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.933 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.906 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.852 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.874 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.851 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR JSD} \hspace{6pt}\huxbpad{6pt}}               &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.889 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.85 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.838 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.871 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.834 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.819 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.865 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.847 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.834 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{PPR cosine} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.92 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.927 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.91 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.885 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.918 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.896 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.812 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.815 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.797 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's cosine} \hspace{6pt}\huxbpad{6pt}}         &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.927 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.926 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.911 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.93 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.924 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.909 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.861 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.872 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.854 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Gravity SVD cosine} \hspace{6pt}\huxbpad{6pt}}    &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.965 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.965 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.95 \hspace{6pt}\huxbpad{6pt}}                                                 &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.955 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.958 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.941 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.873 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.883 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.866 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}

					\multicolumn{1}{!{\huxvb{0, 0, 0}{0}}l!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedright \hspace{6pt} \textbf{Levy's dot} \hspace{6pt}\huxbpad{6pt}}            &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.99 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.998 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.983 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.977 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 1.015 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0.5}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.997 \hspace{6pt}\huxbpad{6pt}}                                                &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.845 \hspace{6pt}\huxbpad{6pt}}                                                  &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.86 \hspace{6pt}\huxbpad{6pt}}                                                   &
					\multicolumn{1}{r!{\huxvb{0, 0, 0}{0}}}{\huxtpad{6pt + 1em}\raggedleft \hspace{6pt} 0.841 \hspace{6pt}\huxbpad{6pt}} \tabularnewline[-0.5pt]


					\hhline{>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|>{\huxb{0, 0, 0}{0.5}}|}
					\arrayrulecolor{black}
				\end{tabular}
			\end{threeparttable}\par\end{centerbox}

	\end{table}


\end{landscape}




%
% BIBLIOGRAPHY
%
\clearpage
\printbibliography{}
\end{document}
